#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import absolute_import, division, print_function

import argparse
import distutils.util
import importlib
import json
import pickle
import time
import warnings
from os.path import join
from pprint import pprint

import nibabel as nib
import numpy as np
from tractseg.data.data_loader_inference import DataLoaderInference
from tractseg.data.data_loader_training import \
    DataLoaderTraining as DataLoaderTraining2D
from tractseg.libs import (direction_merger, exp_utils, img_utils, metric_utils,
                           trainer, utils)
from tractseg.libs.system_config import SystemConfig as C
from tractseg.models.base_model import BaseModel
"""
This module is for training the model. See Readme.md for more details about
training your own model.

How to use this module:

#Run local:
ExpRunner --config=XXX

#Run slurm cluster:
sbatch --job-name=XXX ~/runner.sh   (runner.sh not provided)

#Predicting with new config setup:
ExpRunner --train=False --seg --lw --config=XXX
ExpRunner --train=False --test=True --lw --config=XXX
"""

warnings.simplefilter("ignore", UserWarning)  # hide scipy warnings
warnings.simplefilter("ignore", FutureWarning)  # hide h5py warnings
# hide Cython benign warning
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
# hide Cython benign warning
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")


def test_whole_subject(Config, model, subjects, type):

    metrics = {
        "loss_" + type: [0],
        "f1_macro_" + type: [0],
    }

    # Metrics per bundle
    metrics_bundles = {}
    for bundle in Config.CLASSES:
        metrics_bundles[bundle] = [0]

    for subject in subjects:
        print("{} subject {}".format(type, subject))
        start_time = time.time()

        data_loader = DataLoaderInference(Config, subject=subject)
        img_probs, img_y = trainer.predict_img(
            Config, model, data_loader, probs=True)
        # img_probs_xyz, img_y = DirectionMerger.get_seg_single_img_3_directions(Config, model, subject=subject)
        # img_probs = DirectionMerger.mean_fusion(Config.THRESHOLD, img_probs_xyz, probs=True)

        print("Took {}s".format(round(time.time() - start_time, 2)))

        if Config.EXPERIMENT_TYPE == "peak_regression":
            f1 = metric_utils.calc_peak_length_dice(
                Config,
                img_probs,
                img_y,
                max_angle_error=Config.PEAK_DICE_THR,
                max_length_error=Config.PEAK_DICE_LEN_THR)
            # if f1 for multiple bundles
            peak_f1_mean = np.array([s for s in f1.values()]).mean()
            metrics = metric_utils.calculate_metrics(
                metrics,
                None,
                None,
                0,
                f1=peak_f1_mean,
                type=type,
                threshold=Config.THRESHOLD)
            metrics_bundles = metric_utils.calculate_metrics_each_bundle(
                metrics_bundles,
                None,
                None,
                Config.CLASSES,
                exp_utils.get_bundle_names(Config.CLASSES)[1:],
                f1,
                threshold=Config.THRESHOLD)
        else:
            # Flatten all dims except nrClasses dim
            img_probs = np.reshape(img_probs, (-1, img_probs.shape[-1]))
            img_y = np.reshape(img_y, (-1, img_y.shape[-1]))
            metrics = metric_utils.calculate_metrics(
                metrics,
                img_y,
                img_probs,
                0,
                type=type,
                threshold=Config.THRESHOLD)
            metrics_bundles = metric_utils.calculate_metrics_each_bundle(
                metrics_bundles,
                img_y,
                img_probs,
                Config.CLASSES,
                threshold=Config.THRESHOLD)

    metrics = metric_utils.normalize_last_element(
        metrics, len(subjects), type=type)
    metrics_bundles = metric_utils.normalize_last_element_general(
        metrics_bundles, len(subjects))

    print("WHOLE SUBJECT:")
    pprint(metrics)
    print("WHOLE SUBJECT BUNDLES:")
    pprint(metrics_bundles)

    with open(join(Config.EXP_PATH, "score_" + type + "-set.txt"), "w") as f:
        pprint(metrics, f)
        f.write("\n\nWeights: {}\n".format(Config.WEIGHTS_PATH))
        f.write("type: {}\n\n".format(type))
        pprint(metrics_bundles, f)
    pickle.dump(metrics,
                open(join(Config.EXP_PATH, "score_" + type + ".pkl"), "wb"))
    return metrics


def main():
    parser = argparse.ArgumentParser(
        description=
        "Train a network on your own data to segment white matter bundles.",
        epilog=
        "Written by Pietro Astolfi. Please reference 'Wasserthal et al., TractSeg - Fast and accurate white matter tract segmentation. https://doi.org/10.1016/j.neuroimage.2018.07.070)'"
    )
    #import ipdb;ipdb.set_trace()
    parser.add_argument(
        "-json", help="Name of configuration to use", required=True)

    args = parser.parse_args()

    Config = getattr(
        importlib.import_module("tractseg.experiments.base"), "Config")()

    with open(args.json) as cfg_src:
        cfg = json.load(cfg_src)

    C.HOME = cfg["home"]
    #C.TRACTSEG_DATA_DIR = cfg["tractseg_data_dir"]
    C.DATA_PATH = cfg["tractseg_data_dir"]
    #C.TRACTSEG_WEIGHTS_DIR = cfg["tractseg_weights_dir"]
    C.EXP_PATH = cfg["exp_path"]

    Config.TRAIN = bool(cfg['train'])
    Config.TEST = bool(cfg['test'])
    Config.SEGMENT = bool(cfg["save_predictions"])
    Config.EXP_NAME = cfg["exp_name"]
    Config.MODEL = cfg["model"]
    Config.NUM_EPOCHS = cfg["num_epochs"]
    Config.DATASET = cfg["dataset"]
    Config.CLASSES = cfg["classes"]
    Config.DATASET_FOLDER = cfg["dataset_folder"]
    Config.LABELS_FOLDER = cfg["labels_folder"]
    Config.LOAD_WEIGHTS = bool(cfg["load_weights"])
    Config.TRACTSEG_DIR = cfg["tractseg_dir"]
    Config.TRAIN_SUBJECTS = cfg["train_subjects"]
    Config.VALIDATE_SUBJECTS = cfg["validation_subjects"]
    Config.FEATURES_FILENAME = cfg["features_filename"]
    Config.LABELS_FILENAME = cfg["labels_filename"]
    Config.RESOLUTION = cfg["data_resolution"]
    Config.WEIGHTS_PATH = cfg["weights_path"]
    Config.LOSS_WEIGHT = cfg["loss_weight"]
    Config.LOSS_WEIGHT_LEN = cfg["loss_weight_len"]

    Config.MULTI_PARENT_PATH = join(C.EXP_PATH, Config.EXP_MULTI_NAME)
    Config.EXP_PATH = join(C.EXP_PATH, Config.EXP_MULTI_NAME, Config.EXP_NAME)

    # Note: when test is false, test_subjects is empty and the test phase is
    # not done along training, otherwise for each training epoch the test is
    # executed and the f1 score is computed
    if Config.TEST:
        Config.TEST_SUBJECTS = cfg["test_subjects"]

    #Autoset input dimensions based on settings
    Config.INPUT_DIM = exp_utils.get_correct_input_dim(Config)

    Config.NR_OF_CLASSES = len(Config.CLASSES)


    Config.EXP_PATH = exp_utils.create_experiment_folder(
        Config.EXP_NAME, Config.MULTI_PARENT_PATH, Config.TRAIN)

    print("Hyperparameters:")
    exp_utils.print_Configs(Config)

    # save config
    with open(join(Config.EXP_PATH, "Hyperparameters.txt"), "w") as f:
        Config_dict = {
            attr: getattr(Config, attr)
            for attr in dir(Config)
            if not callable(getattr(Config, attr)) and not attr.startswith("__")
        }
        pprint(Config_dict, f)

    cv = bool(cfg['cross_validation'])

    # TODO CROSSVALIDATION
    if cv:
        trainS=Config.TRAIN_SUBJECTS
        valS=Config.VALIDATE_SUBJECTS
        testS=Config.TEST_SUBJECTS
        subjects = trainS + valS +testS 
        #to have a number of subjects multiple of 5
        subjects = list(utils.chunks(subjects, int(len(subjects)/5)))
        fold=Config.CV_FOLD
         #Config.TRAIN_SUBJECTS, Config.VALIDATE_SUBJECTS, Config.TEST_SUBJECTS = exp_utils.get_cv_fold(Config.CV_FOLD, dataset=Config.DATASET)
        if fold == 0:
           train, validate, test = [0, 1, 2], [3], [4]
        # train, validate, test = [0, 1, 2, 3, 4], [3], [4]
        elif fold == 1:
           train, validate, test = [1, 2, 3], [4], [0]
        elif fold == 2:
           train, validate, test = [2, 3, 4], [0], [1]
        elif fold == 3:
           train, validate, test = [3, 4, 0], [1], [2]
        elif fold == 4:
           train, validate, test = [4, 0, 1], [2], [3]

        subjects = np.array(subjects)
        Config.TRAIN_SUBJECTS, Config.VALIDATE_SUBJECTS, Config.TEST_SUBJECTS = list(subjects[train].flatten()), list(subjects[validate].flatten()), list(subjects[test].flatten())


    if Config.TRAIN:
        # create model
        model = BaseModel(Config)
        # create dataloader
        data_loader = DataLoaderTraining2D(Config)

        print("Training...")
        model = trainer.train_model(Config, model, data_loader)
        
        Config.LOAD_WEIGHTS = True
        Config.WEIGHTS_PATH = ""

    if Config.TEST:
        print("Testing...")
        # load model with the chosen (if none takes the best) weights
        if Config.WEIGHTS_PATH == "":
            Config.WEIGHTS_PATH = exp_utils.get_best_weights_path(
                Config.EXP_PATH, Config.LOAD_WEIGHTS)
            print('Loaded weights at: %s' % Config.WEIGHTS_PATH)
        model = BaseModel(Config)
        test_whole_subject(Config, model, Config.TEST_SUBJECTS, "test")

        if Config.SEGMENT:
            exp_utils.make_dir(join(Config.EXP_PATH, "segmentations"))

            for subject in Config.TEST_SUBJECTS:
                print("Get_segmentation subject {}".format(subject))

                img_seg, img_y = direction_merger.get_seg_single_img_3_directions(
                    Config, model, subject)
                img_seg = direction_merger.mean_fusion(
                    Config.THRESHOLD, img_seg, probs=False)

                # save predictions
                img = nib.Nifti1Image(
                    img_seg.astype(np.uint8),
                    img_utils.get_dwi_affine(Config.DATASET, Config.RESOLUTION))
                nib.save(
                    img,
                    join(Config.EXP_PATH, "segmentations",
                        subject + "_segmentation.nii.gz"))


if __name__ == '__main__':
    main()
