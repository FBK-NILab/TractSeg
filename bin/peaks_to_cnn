#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2017 Division of Medical Image Computing, German Cancer Research Center (DKFZ)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
This module is for training the model. See Readme.md for more details about training your own model.

How to use this module:

#Run local:
ExpRunner --config=XXX

#Run slurm cluster:
sbatch --job-name=XXX ~/runner.sh   (runner.sh not provided)

#Predicting with new config setup:
ExpRunner --train=False --seg --lw --config=XXX
ExpRunner --train=False --test=True --lw --config=XXX
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import warnings
warnings.simplefilter("ignore", UserWarning)    #hide scipy warnings
warnings.simplefilter("ignore", FutureWarning)    #hide h5py warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed") #hide Cython benign warning
warnings.filterwarnings("ignore", message="numpy.ufunc size changed") #hide Cython benign warning
import importlib
import argparse
import pickle
import time
from pprint import pprint
import distutils.util
from os.path import join
import nibabel as nib
import numpy as np

from tractseg.libs import direction_merger
from tractseg.libs import exp_utils
from tractseg.libs import img_utils
from tractseg.libs import metric_utils
from tractseg.libs.system_config import SystemConfig as C
from tractseg.libs import trainer
from tractseg.data.data_loader_training import DataLoaderTraining as DataLoaderTraining2D
from tractseg.data.data_loader_training_3D import DataLoaderTraining as DataLoaderTraining3D
from tractseg.data.data_loader_inference import DataLoaderInference
from tractseg.models.base_model import BaseModel


def main():
    parser = argparse.ArgumentParser(description="Train a network on your own data to segment white matter bundles.",
                                        epilog="Written by Pietro Astolfi. Please reference 'Wasserthal et al. "
                                               "TractSeg - Fast and accurate white matter tract segmentation. "
                                               "https://doi.org/10.1016/j.neuroimage.2018.07.070)'")
    parser.add_argument("-i", nargs='+', metavar="(peaks_in, gt_in)", dest="files_in", help="tuples with: peaks input file (4D nifti diffusion image: (x, y, z, 9), and ground-truth input file (4D nifti diffusion image): (x, y, z, nr_classes)",
                        required=True)
    parser.add_argument("--lw", action="store_true", help="Load weights of pretrained net")
    parser.add_argument("--config", metavar="name", help="Name of configuration to use")
    parser.add_argument("--test", nargs='*', metavar="(peaks_in, gt_in)", help="Test subjects, same format as input", default=False)
    parser.add_argument("--en", metavar="name", help="Experiment name")
    parser.add_argument("--verbose", action="store_true", help="Show more intermediate output", default=True)
    args = parser.parse_args()

    Config = getattr(importlib.import_module("tractseg.experiments.base"), "Config")()
    if args.config:
        # Config.__dict__ does not work properly
        Config = getattr(importlib.import_module("tractseg.experiments.custom." + args.config), "Config")()

    if args.en:
        Config.EXP_NAME = args.en

    Config.TRAIN = True
    Config.TRAIN_SUBJECTS = args.files_in    # here is expected a list of tuples
    Config.VALIDATE_SUBJECTS = []
    if args.test:
        Config.TEST = False
        Config.TEST_SUBJECTS = args.test    # here is expected a list of tuples

    if args.lw:
        Config.LOAD_WEIGHTS = args.lw

    Config.VERBOSE = args.verbose

    Config.MULTI_PARENT_PATH = join(C.EXP_PATH, Config.EXP_MULTI_NAME)
    Config.EXP_PATH = join(C.EXP_PATH, Config.EXP_MULTI_NAME, Config.EXP_NAME)

    if Config.WEIGHTS_PATH == "":
        Config.WEIGHTS_PATH = exp_utils.get_best_weights_path(Config.EXP_PATH, Config.LOAD_WEIGHTS)

    #Autoset input dimensions based on settings
    Config.INPUT_DIM = exp_utils.get_correct_input_dim(Config)

    Config.NR_OF_CLASSES = len(exp_utils.get_bundle_names(Config.CLASSES)[1:])

    Config.EXP_PATH = exp_utils.create_experiment_folder(Config.EXP_NAME, Config.MULTI_PARENT_PATH, Config.TRAIN)

    if Config.DIM == "2D":
        Config.EPOCH_MULTIPLIER = 1
    else:
        Config.EPOCH_MULTIPLIER = 3

    if Config.VERBOSE:
        print("Hyperparameters:")
        exp_utils.print_Configs(Config)

    with open(join(Config.EXP_PATH, "Hyperparameters.txt"), "w") as f:
        Config_dict = {attr: getattr(Config, attr) for attr in dir(Config)
                       if not callable(getattr(Config, attr)) and not attr.startswith("__")}
        pprint(Config_dict, f)

    def test_whole_subject(Config, model, subjects, type):

        metrics = {
            "loss_" + type: [0],
            "f1_macro_" + type: [0],
        }

        # Metrics per bundle
        metrics_bundles = {}
        for bundle in exp_utils.get_bundle_names(Config.CLASSES)[1:]:
            metrics_bundles[bundle] = [0]

        for subject in subjects:
            print("{} subject {}".format(type, subject))
            start_time = time.time()

            data_loader = DataLoaderInference(Config, subject=subject)
            img_probs, img_y = trainer.predict_img(Config, model, data_loader, probs=True)
            # img_probs_xyz, img_y = DirectionMerger.get_seg_single_img_3_directions(Config, model, subject=subject)
            # img_probs = DirectionMerger.mean_fusion(Config.THRESHOLD, img_probs_xyz, probs=True)

            print("Took {}s".format(round(time.time() - start_time, 2)))

            if Config.EXPERIMENT_TYPE == "peak_regression":
                f1 = metric_utils.calc_peak_length_dice(Config, img_probs, img_y,
                                                        max_angle_error=Config.PEAK_DICE_THR,
                                                        max_length_error=Config.PEAK_DICE_LEN_THR)
                peak_f1_mean = np.array([s for s in f1.values()]).mean()  # if f1 for multiple bundles
                metrics = metric_utils.calculate_metrics(metrics, None, None, 0, f1=peak_f1_mean,
                                                         type=type, threshold=Config.THRESHOLD)
                metrics_bundles = metric_utils.calculate_metrics_each_bundle(metrics_bundles, None, None,
                                                                             exp_utils.get_bundle_names(Config.CLASSES)[1:],
                                                                             f1, threshold=Config.THRESHOLD)
            else:
                img_probs = np.reshape(img_probs, (-1, img_probs.shape[-1]))  #Flatten all dims except nrClasses dim
                img_y = np.reshape(img_y, (-1, img_y.shape[-1]))
                metrics = metric_utils.calculate_metrics(metrics, img_y, img_probs, 0,
                                                         type=type, threshold=Config.THRESHOLD)
                metrics_bundles = metric_utils.calculate_metrics_each_bundle(metrics_bundles, img_y, img_probs,
                                                                             exp_utils.get_bundle_names(Config.CLASSES)[1:],
                                                                             threshold=Config.THRESHOLD)

        metrics = metric_utils.normalize_last_element(metrics, len(subjects), type=type)
        metrics_bundles = metric_utils.normalize_last_element_general(metrics_bundles, len(subjects))

        print("WHOLE SUBJECT:")
        pprint(metrics)
        print("WHOLE SUBJECT BUNDLES:")
        pprint(metrics_bundles)

        with open(join(Config.EXP_PATH, "score_" + type + "-set.txt"), "w") as f:
            pprint(metrics, f)
            f.write("\n\nWeights: {}\n".format(Config.WEIGHTS_PATH))
            f.write("type: {}\n\n".format(type))
            pprint(metrics_bundles, f)
        pickle.dump(metrics, open(join(Config.EXP_PATH, "score_" + type + ".pkl"), "wb"))
        return metrics


    model = BaseModel(Config)
    if Config.DIM == "2D":
        data_loader = DataLoaderTraining2D(Config)
    else:
        data_loader = DataLoaderTraining3D(Config)

    print("Training...")
    model = trainer.train_model(Config, model, data_loader)

    # After Training
    if Config.TRAIN:
        # Have to load other weights, because after training it has the weights of the last epoch
        print("Loading best epoch: {}".format(Config.BEST_EPOCH))
        Config.WEIGHTS_PATH = Config.EXP_PATH + "/best_weights_ep" + str(Config.BEST_EPOCH) + ".npz"
        Config.LOAD_WEIGHTS = True
        model.load_model(join(Config.EXP_PATH, Config.WEIGHTS_PATH))
        model_test = model
    else:
        # Weight_path already set to best model (when reading program parameters) -> will be loaded automatically
        model_test = model

    if Config.TEST:
        test_whole_subject(Config, model_test, Config.TEST_SUBJECTS, "test")

if __name__ == '__main__':
    main()
